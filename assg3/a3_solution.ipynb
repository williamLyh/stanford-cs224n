{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a) Adam Optimizer\n",
    "\n",
    "    (i) Momentum $\\mathbf{m}$: $\\mathbf{m}$ is the rolling average of the gradient. Because Stochastic Gradient Descent uses randomly picked data to find the gradient, for each step the direction of the gradient could be very different. If we average the new gradient with the rolling average $\\mathbf{m}$, the resulting gradient direction would point more directly to the minimum, just like taking account of more training data to calculate this gradient, but faster.\n",
    "    \n",
    "    (ii) Adaptive learning rates $\\mathbf{v}$: In $\\mathbf{v}$, when the new gradient magnitude is small, which means it is closed to the minimum, the learning step will be larger. In another words, if correct, correct more.\n",
    "    \n",
    "   b) Dropout \n",
    "   \n",
    "   (i) Express $\\gamma$ in terms of $p_{drop}$:\n",
    "   \n",
    "\\begin{align}\n",
    "E[h_{drop,i}] = E[\\gamma d_i h_i] \\\\\n",
    "\\gamma = \\frac{1}{1-p_{drop}}\n",
    "\\end{align}\n",
    "    \n",
    "    (ii) Why Dropout during training, but not evaluation?\n",
    "    Because Dropout is a regulation technique, which means it is used to prevent overfitting. During evaluation, we don't tune the parameter values any more. Therefore, there's no need to dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
